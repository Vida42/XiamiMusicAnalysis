
## `Xiami` is a web crawler written by myself in python during Sep. - Oct. 2017


## `Scrapy` is the Web Crawling Framework edited by myself during Sep. - Oct. 2017


function of `xst.py` :

For the user that I failed to collect their whole favorite libraries, I found they usually have large size of favorite libraries. So the program would return Timeout frequently. I wrote another version to tackle this question. I created another file to store the current page in which the program return Timeout. So the program could automaticlly kept collecting from the break page in next loop.

function of`rearrange2.py`:

Since I ran the WebCrawler in several Computers, so I need to combine the results after I got the results from each computer.
